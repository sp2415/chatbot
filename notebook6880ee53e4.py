# -*- coding: utf-8 -*-
"""notebook6880ee53e4

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/notebook6880ee53e4-d8da2158-69e3-48c9-b54f-21d21f4a5d48.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20250319/auto/storage/goog4_request%26X-Goog-Date%3D20250319T105735Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D0c678604178c9f4ddc99d9917472bd615f2c4d8bd8610a5dba51e8d7e72c10565590ba5790d262100f082ff05a10d0e8c1d67f904e6bfe078053f964d2340f2d1df5540cf27eacb5aa122c765993b605e6ced85983af311b61906736fe9a0cdbc51d5ac73e5e352bac3ac30b0a4375f53e68f1ce643f9470113d602b87b7099d268e7f9c0804c1c7fba05a756c6afbef384eaacf63d23a22770642811584ed857b1d6f8540b7d2e3a1d4b58d26620f31e1856347968063e683de2e73ebd7a5e9b10cb6b0ca3ebf7141620c3973b817c81f5554351d0a537b4ec1a1b0cf7c7aa918f7d4840ed51a6116312f7a65efc608c367605f2918f657c7ec05326c37b60e
"""

# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE
# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.
import kagglehub
kagglehub.login()

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

shubhamf73_data009_path = kagglehub.dataset_download('shubhamf73/data009')
shubhamf73_data0010_path = kagglehub.dataset_download('shubhamf73/data0010')

print('Data source import complete.')

!pip install fire
!pip install pydantic
!pip install pytorch-lightning
!pip install torch
!pip install tqdm
!pip install transformers
!pip install safetensors
!pip install datasets
!pip install git+https://github.com/huggingface/peft.git@e536616888d51b453ed354a6f1e243fecb02ea08
!pip install peft
!pip install sacrebleu
!pip install data_loading

!pip install pytorch-lightning

from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, Adafactor
from pytorch_lightning import seed_everything
import argparse
import functools
import os
import pytorch_lightning as pl
import torch
from peft import LoraConfig, TaskType, get_peft_model
from pytorch_lightning.callbacks import ModelCheckpoint
from pytorch_lightning.strategies import FSDPStrategy
from torch.distributed.fsdp import (MixedPrecision, FullyShardedDataParallel, StateDictType, FullStateDictConfig,)
from torch.distributed.fsdp.wrap import transformer_auto_wrap_policy
from transformers.models.t5.modeling_t5 import T5Block
from torch.utils.data import DataLoader
import json
import random
from collections import Counter
from pathlib import Path
from typing import List, Optional, Dict
from datasets import load_dataset
from fire import Fire
from pydantic import BaseModel, Field
from torch.utils.data import Dataset
from tqdm import tqdm
from transformers import PreTrainedTokenizer, BatchEncoding, AutoTokenizer

# class FindingTokensAnalyze (BaseModel, arbitrary_types_allowed=True):
#   name: str
#   tokenizer: Optional[PreTrainedTokenizer] = None

from transformers import PreTrainedTokenizer, PreTrainedTokenizerFast
from typing import Union


class FindingTokensAnalyze(BaseModel,arbitrary_types_allowed=True):
  name: str
  tokenizer: Optional[Union[PreTrainedTokenizer, PreTrainedTokenizerFast]] = None  # Accept both types
  def load(self):
    if self.tokenizer is None:
      self.tokenizer = AutoTokenizer.from_pretrained(
        self.name, model_max_length = 99999
        )

  def run(self, texts: List[str], limit: int = 0) -> Dict[str, float]:
    if limit:
      texts = texts[:limit]

    self.load()
    tokens = self.tokenizer (texts).input_ids
    lengths = sorted(len(lst) for lst in tokens)
    info = dict(min=lengths[0], max=lengths[-1], median=lengths [len (lengths) // 2])
    info.update({"95_percentile": lengths [round (len(lengths) * 0.95)]})
    return info

class TextToTextSample(BaseModel):
  source: str
  target: str

class TextToTextData(BaseModel):
  samples: List[TextToTextSample]
  @classmethod
  def load(cls, path: str):
    with open(path) as f:
      all_lines = tqdm(f.readlines(), desc = path)
      samples = [TextToTextSample (**json. loads (line)) for line in all_lines]
      return cls(samples=samples)

  def save(self, path: str):
    Path(path).parent.mkdir(exist_ok=True, parents=True)
    with open(path, "w") as f:
      for sample in self.samples:
        print(sample.json(), file=f)

  def analyze(self, num: int = 10, tokenizer_name: str = "t5-base"):
    random.seed(num)
    for sample in random.sample(self.samples, k=num):
      print(sample.model_dump_json(indent=2))

    token_checker = FindingTokensAnalyze(name=tokenizer_name)
    info = dict(
      total_samples = len(self.samples),
      source=str(token_checker.run([sample.source for sample in self.samples])),
      target=str(token_checker.run([sample.target for sample in self.samples])),
    )
    print(json.dumps (info, indent=2))

class MentalSample(BaseModel):
  Context: str
  Response: str

class Mental_Data(BaseModel):
    samples: List[MentalSample]

    @classmethod
    def load(cls, path: str):
        with open(path) as f:
            raw = json.load(f)
        return cls(samples=[MentalSample(**r) for r in raw])

    def save(self, path: str):
        raw = [sample.dict() for sample in self.samples]
        Path(path).parent.mkdir(exist_ok=True, parents=True)
        with open(path, "w") as f:
            json.dump(raw, f)

    def as_data(self) -> TextToTextData:
        self.analyze()
        samples = []
        for raw in self.samples:
            if raw.Context.strip():
                source = raw.Context
                samples.append(TextToTextSample(source=source, target=raw.Response))
        return TextToTextData(samples=samples)

    def analyze(self):
      info = dict(
          Context_Sample = len(self.samples),
          Response_Sample= sum(sample.Context.strip() != "" for sample in self.samples),
      )
      print(json.dumps(info, indent=2))


class TextToTextDataset(Dataset):
    def __init__(
        self,
        path: str,
        tokenizer: PreTrainedTokenizer,
        max_source_length: int,
        max_target_length: int,
    ):
        self.max_source_length = max_source_length
        self.max_target_length = max_target_length
        self.tokenizer = tokenizer
        self.data = TextToTextData.load(path)

    def __len__(self) -> int:
        return len(self.data.samples)

    def tokenize(self, text: str, is_source: bool) -> BatchEncoding:
        x = self.tokenizer(
            text,
            max_length=self.max_source_length if is_source else self.max_target_length,
            padding="max_length",
            truncation=not is_source,
            return_tensors="pt",
        )

        """
        T5 truncates on right by default, but we can easily truncate on left
        for the encoder input as there is no special token on the left side
        """
        if is_source:
            assert x.input_ids.ndim == 2
            assert x.input_ids.shape == x.attention_mask.shape
            length = x.input_ids.shape[1]
            start = max(length - self.max_source_length, 0)
            x.input_ids = x.input_ids[:, start:]
            x.attention_mask = x.attention_mask[:, start:]
            assert x.input_ids.shape[1] == self.max_source_length
        return x

    def __getitem__(self, i: int) -> dict:
      x = self.tokenize(self.data.samples[i].source, is_source=True)
      y = self.tokenize(self.data.samples[i].target, is_source=False)

      return {
          "source_ids": x.input_ids.squeeze(),
          "source_mask": x.attention_mask.squeeze(),
          "target_ids": y.input_ids.squeeze(),
          "target_mask": y.attention_mask.squeeze(),
      }

    def to_human_readable(self, raw: dict) -> dict:
        source = self.tokenizer.decode(raw["source_ids"])
        target = self.tokenizer.decode(raw["target_ids"])
        return dict(source=source, target=target)

# def preprocess_mental(
#     path_in: str = "data/mental.json", path_out: str = "data/train.json"
# ):
#     data = Mental_Data.load(path_in).as_data()
#     data.analyze()
#     data.save(path_out)

def preprocess_mental(path_in: str = "data/mental.json", path_out: str = "data/train.json"):
    data = Mental_Data.load(path_in).as_data()

    # Pass tokenizer explicitly
    tokenizer = AutoTokenizer.from_pretrained("t5-base")
    token_checker = FindingTokensAnalyze(name="t5-base", tokenizer=tokenizer)

    info = dict(
        total_samples=len(data.samples),
        source=str(token_checker.run([sample.source for sample in data.samples])),
        target=str(token_checker.run([sample.target for sample in data.samples])),
    )

    print(json.dumps(info, indent=2))
    data.save(path_out)

import sys
from data_loading import TextToTextDataset

os.environ["TOKENIZERS_PARALLELISM"] = "true"

def get_args(raw_args):
    parser = argparse.ArgumentParser()
    parser.add_argument("--model_name_or_path", type=str, default="google-t5/ts-base")
    parser.add_argument("--max_source_length", type=int, default=400)
    parser.add_argument("--max_target_length", type=int, default=160)
    parser.add_argument("--data_path", type=str, default="data/train.json")
    parser.add_argument("--train_epochs", type=int, default=5)
    parser.add_argument("--train_batch_size", type=int, default=16)
    parser.add_argument("--gradient_accumulation_steps", type=int, default=1)
    parser.add_argument("--learning_rate", type=float, default=3e-3)
    parser.add_argument("--seed", type=int, default=42)
    parser.add_argument("--weight_decay", type=float, default=0.0)
    parser.add_argument("--output_dir", type=str, default="")
    parser.add_argument("--use_compile", action="store_true")
    parser.add_argument("--use_gradient_checkpointing", action="store_true")
    parser.add_argument("--use_fsdp", action="store_true")
    parser.add_argument("--use_16", action="store_true")
    parser.add_argument("--debug", action="store_true")
    parser.add_argument("--use_lora", action="store_true")
    args = parser.parse_args(raw_args)
    return args

class LightningModel(pl.LightningModule):
    def __init__(self, hparams):
        super().__init__()
        self.save_hyperparameters(hparams)
        print(self.hparams)

        self.model = AutoModelForSeq2SeqLM.from_pretrained(
            self.hparams.model_name_or_path
        )

        print(dict(orig_state_dict=len(self.model.state_dict())))

        # if self.hparams.use_lora:
        #     # https://github.com/huggingface/peft/blob/main/examples/conditional_generation/peft_lora_seq2seq.ipynb
        #     peft_config = LoraConfig(
        #         task_type=TaskType.SEQ_2_SEQ_LM,
        #         inference_mode=False,
        #         r=8,
        #         lora_alpha=8,
        #         lora_dropout=0.05,
        #         adaptive_r = True,
        #         min_r = 4,
        #         max_r = 16
        #     )


        # if self.hparams.use_lora:
        #     # Instead of LoRA, using Prefix Tuning configuration
        #     from peft import PrefixTuningConfig, get_peft_model
        #     prefix_config = PrefixTuningConfig(
        #         task_type=TaskType.SEQ_2_SEQ_LM,
        #         num_virtual_tokens=20,       # number of virtual tokens (tweak this as needed)
        #         # prefix_tuning_init_std=0.02,   # initialization std; you can experiment with this
        #         # You can add other hyperparameters if available and needed
        #     )
        #     self.model = get_peft_model(self.model, prefix_config)


            # self.model = get_peft_model(self.model, peft_config)

        if self.hparams.use_compile:
            self.model = torch.compile(self.model)

        if self.hparams.use_gradient_checkpointing:
            self.model.gradient_checkpointing_enable()

        self.tokenizer = AutoTokenizer.from_pretrained(self.hparams.model_name_or_path)

    def forward(
        self,
        input_ids,
        attention_mask=None,
        decoder_input_ids=None,
        decoder_attention_mask=None,
        labels=None,
    ):
        return self.model(
            input_ids,
            attention_mask=attention_mask,
            decoder_input_ids=decoder_input_ids,
            decoder_attention_mask=decoder_attention_mask,
            labels=labels,
        )

    def _step(self, batch):
      lm_labels = batch["target_ids"]
      lm_labels[lm_labels[:, :] == self.tokenizer.pad_token_id] = -100

      outputs = self.model(
        input_ids=batch["source_ids"],
        attention_mask=batch["source_mask"],
        labels=lm_labels,
        decoder_attention_mask=batch["target_mask"],
      )

      loss = outputs[0]
      return loss

    def training_step(self, batch, batch_idx):
      loss = self._step(batch)
      self.log("loss", loss, on_step=True, prog_bar=True, rank_zero_only=True)
      return loss

    # def configure_optimizers(self):
    #   no_decay = ["bias", "LayerNorm.weight"]
    #   params = self.trainer.model.named_parameters()
    #   optimizer_grouped_parameters = [
    #     {
    #       "params": [p for n, p in params if not any(nd in n for nd in no_decay)],
    #       "weight_decay": self.hparams.weight_decay,
    #     },
    #     {
    #       "params": [p for n, p in params if any(nd in n for nd in no_decay)],
    #       "weight_decay": 0.0,
    #     },
    #   ]

    #   # noinspection PyTypeChecker
    #   optimizer = Adafactor(
    #   optimizer_grouped_parameters,
    #   lr=self.hparams.learning_rate,
    #   relative_step=False,
    #   )
    #   return [optimizer]

    def configure_optimizers(self):
        no_decay = ["bias", "LayerNorm.weight"]
        params = self.model.named_parameters()  # Use self.model instead of self.trainer.model
        optimizer_grouped_parameters = [
            {
                "params": [p for n, p in params if not any(nd in n for nd in no_decay)],
                "weight_decay": self.hparams.weight_decay,
            },
            {
                "params": [p for n, p in params if any(nd in n for nd in no_decay)],
                "weight_decay": 0.0,
            },
        ]
        optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=self.hparams.learning_rate)
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)

        return {
            "optimizer": optimizer,
            "lr_scheduler": {
                "scheduler": scheduler,
                "interval": "epoch",
                "frequency": 1,
                "monitor": "loss",  # You can log loss as your metric here
            },
        }


    def train_dataloader(self):
      dataset = TextToTextDataset(
        path=self.hparams.data_path,
        max_source_length=self.hparams.max_source_length,
        max_target_length=self.hparams.max_target_length,
        tokenizer=self.tokenizer,
    )

      return DataLoader(
        dataset,
        batch_size=self.hparams.train_batch_size,
        drop_last=True,
        shuffle=True,
    )

def main(args):
    torch.set_float32_matmul_precision("high")
    #args = get_args(raw_args)
    seed_everything(args.seed)
    model = LightningModel(args)

    saver = ModelCheckpoint(
        verbose=True,
        dirpath=args.output_dir,
        save_weights_only=True,
    )

    strategy = "auto"
    if args.use_fsdp:
        # https://pytorch.org/blog/efficient-large-scale-training-with-pytorch/
        # https://pytorch.org/docs/stable/advanced/model_parallel.html
        strategy = MyFSDPStrategy(
            auto_wrap_policy=functools.partial(
                transformer_auto_wrap_policy,
                transformer_layer_cls={T5Block},
            ),
            mixed_precision=MixedPrecision(
                param_dtype=torch.float16,
                reduce_dtype=torch.float16,
                buffer_dtype=torch.float16,
            ),
            activation_checkpointing=T5Block,
            cpu_offload=True,
        )

    trainer = pl.Trainer(
      precision="32",
      accelerator="cuda",
      # devices = 2,
      strategy=strategy,
      accumulate_grad_batches=1 if args.debug else args.gradient_accumulation_steps,
      default_root_dir=args.output_dir,
      gradient_clip_val=None if args.use_fsdp else 1.0,
      max_epochs=args.train_epochs,
      callbacks=[saver],
      logger=False,
      overfit_batches=10 if args.debug else 0,
    )
    trainer.fit(model)
    return model

import torch
from fire import Fire
from huggingface_hub import HfApi
from lightning_fabric import seed_everything

# from training import LightningModel

def test_model(path, prompt, max_length: int = 160, device: str = "gpu"):
    if not prompt:
        prompt = "Write a short email to show that 42 is the optimal seed for training neural networks"

    model: LightningModel = LightningModel.load_from_checkpoint(path)
    tokenizer = model.tokenizer
    input_ids = tokenizer(prompt, return_tensors="pt").input_ids

    seed_everything(model.hparams.seed)

    with torch.inference_mode():
        model.model.eval()
        model = model.to(device)
        input_ids = input_ids.to(device)
        outputs = model.model.generate(
            input_ids=input_ids, max_length=max_length, do_sample=True
        )

    print(tokenizer.decode(outputs[0]))

def export_checkpoint(path: str, path_out: str):
    model = LightningModel.load_from_checkpoint(path)
    model.model.save_pretrained(path_out)
    model.tokenizer.save_pretrained(path_out)

preprocess_mental(path_in="/kaggle/input/data0010/mental_health_counseling_conversations_1 (1).json", path_out="/content/mental_processed.json")

# optimizer = torch.optim.AdamW(model.parameters(), lr=0.0025)
# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)

# args = get_args(["--model_name_or_path", "google-t5/t5-base", "--data_path", "/content/working/mental_processed.json","--learning_rate", "0.0025", "--use_lora", "True", "--output_dir", "kaggle/working/"])
# model = main(args)

args = get_args([
    "--model_name_or_path", "google-t5/t5-base",
    "--data_path", "/content/mental_processed.json",
    "--learning_rate", "0.003",
    "--output_dir", "kaggle/working/"
    # "--use_lora"
])
model = main(args)

from datasets import load_dataset

import torch
from nltk.translate.bleu_score import corpus_bleu

# Load the evaluation dataset
eval_dataset = TextToTextDataset(
    path=args.data_path,
    tokenizer=model.tokenizer,
    max_source_length=args.max_source_length,
    max_target_length=args.max_target_length,
)

# Define the number of samples to remove
num_samples_to_remove = 1500  # Assuming this is the number of samples to remove
eval_dataset.data.samples = eval_dataset.data.samples[500:520]

# Define a function to generate responses using the model
def generate_responses(model, dataset):
    generated_responses = []
    for i in range(len(dataset)):
        input_ids = dataset[i]["source_ids"].unsqueeze(0)
        attention_mask = dataset[i]["source_mask"].unsqueeze(0)

        output_ids = model.model.generate(
            input_ids=input_ids,
            attention_mask=attention_mask,
            max_length=args.max_target_length,
            num_beams=4,
            early_stopping=True,
            decoder_start_token_id=model.tokenizer.pad_token_id,
        )

        generated_responses.append(output_ids.squeeze(0).tolist())
    return generated_responses

# Generate responses using the finetuned model
generated_responses = generate_responses(model, eval_dataset)

# Convert generated responses back to text
generated_texts = [model.tokenizer.decode(ids, skip_special_tokens=True) for ids in generated_responses]

# Load reference target texts
target_texts = [sample.target for sample in eval_dataset.data.samples]

# Compute BLEU score
bleu_score = corpus_bleu([[text] for text in target_texts], generated_texts)
print("BLEU Score:", bleu_score)
# print(f"Generated Texts: {generated_texts}")
# print(f"Target texts:{target_texts}")

# Save the model
model.model.save_pretrained("/content/saved_model") # Save to a specific location
model.tokenizer.save_pretrained("/content/saved_model") # Save tokenizer
print("Model saved to /content/saved_model")

# prompt: add code below to test saved model with custom input

from transformers import pipeline

# Load the saved model
generator = pipeline('text2text-generation', model='/content/saved_model', tokenizer='/content/saved_model')

# Get custom input from the user
custom_input = input("I am feeling very stressed about my upcomming presentation")

# Generate text using the loaded model
generated_text = generator(custom_input, max_length=160)

# Print the generated text
print(generated_text[0]['generated_text'])

!pip install streamlit pyngrok

# Create a `app.py` file
app_code = """
import streamlit as st
import torch
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

# Load the saved model and tokenizer
model_path = "/content/saved_model"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForSeq2SeqLM.from_pretrained(model_path)

# Set the model to evaluation mode
model.eval()

# Streamlit UI
st.title("Mental Health Support Chatbot")

# Input text box for user input
user_input = st.text_input("HI, How may I help you?: ", "")

# Generate response when the user inputs text
if user_input:
    # Tokenize the input
    input_ids = tokenizer(user_input, return_tensors="pt").input_ids

    # Generate response
    with torch.no_grad():
        output_ids = model.generate(input_ids, max_length=160, num_beams=4, early_stopping=True)

    # Decode the output tokens to text
    response = tokenizer.decode(output_ids[0], skip_special_tokens=True)

    # Display the response
    st.text_area("Bot:", value=response, height=100)
"""

# Write the Streamlit app to a Python file
with open("app.py", "w") as f:
    f.write(app_code)

# Start Streamlit in the background
get_ipython().system_raw('streamlit run app.py &')

# Use ngrok to create a public URL
from pyngrok import ngrok

ngrok.kill()

!ngrok authtoken 2tB6s45MGSybRW0gdePY8Uar8WT_4kdsMHh67AogfbZXaj5Gh

# Kill any previous tunnels (to avoid port conflicts)
#ngrok.kill()

# Create a new tunnel
public_url = ngrok.connect("http://localhost:8501")

print(f"Streamlit Public URL: {public_url}")

